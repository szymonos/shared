{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Machine Learning and Pipeline SDK-specific imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SDK: v1.42.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from azureml.core import VERSION, Workspace\n",
        "from azureml.data import HDFSOutputDatasetConfig\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import SynapseSparkStep\n",
        "\n",
        "# Check core SDK version number\n",
        "print(f\"SDK: v{VERSION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mldevorgrunprod\n",
            "AZ-RG-DSL-MLDEVORGRUNPROD\n"
          ]
        }
      ],
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datastore workspaceblobstore will be used\n",
            "Uploading an estimated of 1 files\n",
            "Target already exists. Skipping upload for Titanic.csv\n",
            "Uploaded 0 files\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "$AZUREML_DATAREFERENCE_workspaceblobstore"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use the default blob storage\n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print('Datastore {} will be used'.format(def_blob_store.name))\n",
        "\n",
        "# We are uploading a sample file in the local directory to be used as a datasource\n",
        "file_name = \"Titanic.csv\"\n",
        "def_blob_store.upload_files(files=[f\"./data/{file_name}\"], overwrite=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tabular dataset as input\n",
            "file dataset as input\n",
            "register output as file dataset\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "print(\"tabular dataset as input\")\n",
        "titanic_tabular_dataset = Dataset.Tabular.from_delimited_files(path=[(def_blob_store, file_name)])\n",
        "input1 = titanic_tabular_dataset.as_named_input(\"tabular_input\")\n",
        "\n",
        "print(\"file dataset as input\")\n",
        "titanic_file_dataset = Dataset.File.from_files(path=[(def_blob_store, file_name)])\n",
        "input2 = titanic_file_dataset.as_named_input(\"file_input\").as_hdfs()\n",
        "\n",
        "print(\"register output as file dataset\")\n",
        "from azureml.data import HDFSOutputDatasetConfig\n",
        "\n",
        "output = HDFSOutputDatasetConfig(destination=(def_blob_store, \"test\")).register_on_complete(name=\"registered_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic_tabular_dataset = Dataset.Tabular.from_delimited_files(path=[(def_blob_store, file_name)])\n",
        "titanic_file_dataset = Dataset.File.from_files(path=[(def_blob_store, file_name)])\n",
        "\n",
        "step1_input1 = titanic_tabular_dataset.as_named_input(\"tabular_input\")\n",
        "step1_input2 = titanic_file_dataset.as_named_input(\"file_input\").as_hdfs()\n",
        "step1_output = HDFSOutputDatasetConfig(destination=(def_blob_store, \"test\")).register_on_complete(\n",
        "    name=\"registered_dataset\"\n",
        ")\n",
        "\n",
        "step2_input = step1_output.as_input(\"step2_input\").as_download()\n",
        "\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "env = Environment(name=\"myenv\")\n",
        "env.python.conda_dependencies.add_pip_package(\"azureml-core==1.38.0\")\n",
        "\n",
        "step_1 = SynapseSparkStep(\n",
        "    name=\"synapse-spark\",\n",
        "    file=\"dataprep.py\",\n",
        "    source_directory=\"./code\",\n",
        "    inputs=[step1_input1, step1_input2],\n",
        "    outputs=[step1_output],\n",
        "    arguments=[\"--tabular_input\", step1_input1, \"--file_input\", step1_input2, \"--output_dir\", step1_output],\n",
        "    compute_target='SparkPoolSmall',\n",
        "    driver_memory=\"7g\",\n",
        "    driver_cores=4,\n",
        "    executor_memory=\"7g\",\n",
        "    executor_cores=2,\n",
        "    num_executors=1,\n",
        "    environment=env,\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[step_1])\n",
        "pipeline_run = pipeline.submit(\"synapse-pipeline\", regenerate_outputs=True)\n",
        "\n",
        "# get pipeline status\n",
        "pipeline_run.wait_for_completion()\n",
        "for step_run in pipeline_run.get_children():\n",
        "    print(f\"{step_run.name}: {step_run.get_metrics()}\")"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "yunzhan"
      }
    ],
    "interpreter": {
      "hash": "85b8b013d4081a2e36524e200b3811125f7c0ca467990ce54cfe9e8c8a85bf71"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('aml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
